[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Investigating Temperature and Income in San Jose\n\n\n\nMEDS\n\n\nStats\n\n\n\nStatistical data analysis project concering the relationship between heat and income in San Jose, CA, for the graduate course EDS 222\n\n\n\nNaomi Moraes\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\n\nPython\n\n\nMEDS\n\n\nGeospatial\n\n\n\nUsing Python to Examine the Santa Barbara Thomas Fire\n\n\n\nNaomi Moraes\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblog post title\n\n\n\nQuarto\n\n\nMEDS\n\n\nSanFran\n\n\n\na short catchy description of the blog post\n\n\n\nNaomi Moraes\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nThis is a website I’m making for grad school. I will be posting professional repositories and the like here!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Naomi Moraes",
    "section": "",
    "text": "Having lived in multiple states and abroad, Naomi Moraes has appreciated the global interconnectivity of local environmental issues and solutions, from an early age. Her interest in mitigating climate change impacts led her to complete her undergraduate degree in Economics and Environmental Studies at the University of Michigan, Ann Arbor, where she graduated with a Bachelor of Arts in 2021."
  },
  {
    "objectID": "blog/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html",
    "href": "blog/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html",
    "title": "nmoraescommit",
    "section": "",
    "text": "# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read in data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', compression = 'zip')\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', compression = 'zip')\n\n# Concatenate both dataframes\naqi = pd.concat([aqi_17, aqi_18])\n\n# Simplify column names\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Create filter Santa Barbara df with dropped columns\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code','county_code'], axis = 1)\n               \n# Change 'date' to datetime and set to index\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n               \n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n               \n# Create column with new variable 'rolling_variable'\naqi_sb['five_day_average'] = rolling_average\n               \n# Create plot with daily AQI and 5-day averages\ndaily_aqi_plot = aqi_sb[['aqi','five_day_average']].plot(title = 'Daily and 5-Day Average AQI in Santa Barbara County',\n                                  xlabel = 'Date',\n                                  ylabel = 'AQI')"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "Importing all relevant libraries - for all excersizes in this blog post.\n\n# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport rioxarray as rioxr\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\n\n\n\n\n# Read in data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', compression = 'zip')\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', compression = 'zip')\n\n# Concatenate both dataframes\naqi = pd.concat([aqi_17, aqi_18])\n\n# Simplify column names\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Create filter Santa Barbara df with dropped columns\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code','county_code'], axis = 1)\n               \n# Change 'date' to datetime and set to index\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n               \n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n               \n# Create column with new variable 'rolling_variable'\naqi_sb['five_day_average'] = rolling_average\n               \n# Create plot with daily AQI and 5-day averages\ndaily_aqi_plot = aqi_sb[['aqi','five_day_average']].plot(title = 'Daily and 5-Day Average AQI in Santa Barbara County',\n                                  xlabel = 'Date',\n                                  ylabel = 'AQI')\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe purpose of this notebook is to explore, clean, and analyze the California fire perimeter shapefile, published by CAL FIRE. This is to obtain the Thomas Fire perimeter boundary for use in “hwk4-task2-false-color-MORAES”.\nHighlights: Working with this dataset was illuminating in looking at how state agencies store fire data and the aspects state agecies deem important to record, the fact that there is a start and end date to the observations, as well as learning how to store updated shape files. I consider the practice in setting up an entirely new project from scratch to be quite valuable, along with the process of independantly learning to access data for a continous workflow, important.\nAbout the data: This dataset was published and maintained by CAL FIRE, but accessed through Data.gov. The statewide fire history geospatial dataset is updated annually from the previous fire season, during spring, from units across the state and cooperating agencies. The first version was released in May 2015 - according the the CalFire site.\n\n\n\n\n\n\n# Display all columns when looking at dataframes\npd.set_option(\"display.max.columns\", None)\n\n\n# Create data filepath\nfp = os.path.join('data','California_Fire_Perimeters_(all).shp')\n\n# Create dataframe for CA fire perimeter shapefile\nca_fire_perimeter = gpd.read_file(fp)\n\n\n\n\nIn this section we will take a preliminary look at the imported fire perimeter data - in order to understand how to extract the Thomas Fire perimenter data.\n\n# Check dataframe head\nca_fire_perimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNaN\nNaN\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNaN\nNaN\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNaN\nNaN\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNaN\nNaN\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNaN\nNaN\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNaN\nNaN\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Check CRS - and type\nca_fire_perimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nAfter the exploration of the California perimeter shape file, imported into this notebook, I see that is has 22,260 fire perimeter observations with columns for varied descriptive markers including: year, fire name, alarm date, and geometery. I observe that some of the column data types int64 and float64, however, I may want to change the date columns into datetime objects for manipulation. The CRS of this shapefile is a projected coordinate reference system, EPSG: 3857 and is a popular crs for web mapping services.\n\n\n\nNext, I will clean the data for easier data manipulation.\n\n# Simplify column names by replacing spaces and no capitilization\nca_fire_perimeter.columns = (ca_fire_perimeter.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Make dates into DateTime object\nca_fire_perimeter.alarm_date = pd.to_datetime(ca_fire_perimeter.alarm_date)\nca_fire_perimeter.cont_date = pd.to_datetime(ca_fire_perimeter.cont_date)\n\n\n\n\nHere, I will select for the Thomas Fire Boundary (2017), and save it as a new geospatial file.\n\n# Select Thomas Fire in 2017\nthomas_fire_boundary = ca_fire_perimeter[(ca_fire_perimeter['alarm_date'] &gt; '2016-12-31') & \n                                         (ca_fire_perimeter['alarm_date'] &lt; '2018-01-01') &\n                                         (ca_fire_perimeter['fire_name'] == 'THOMAS')]\n\n\n# View dataframe\nthomas_fire_boundary\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNaN\nNaN\nNaN\nNaN\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\n\n# Save dataframe as geospatial file in /data folder\nthomas_fire_boundary.to_file('data/thomas_fire_boundary.geojson', driver = 'GeoJSON')\n\nI chose to convert the alarm_date and cont_date variables into DateTime objects, and wanted them to retain that data type. As I would need to convert DateTime objects back into strings to save as shapefile, I chose to store the new data frame as a GeoJSON file. I have saved the data in the ‘data/’ folder common to this project.\n\n\n\n\n\nIn a new notebook, I have created a path to the Thomas Fire perimeter data saved in the previous step.\n\n\n\n# Set up file paths\nland_fp = os.path.join('data', 'landsat8-2018-01-26-sb-simplified.nc')\nthomas_fp = os.path.join('data', 'thomas_fire_boundary.geojson')\n\n# Import landsat data\nlandsat = rioxr.open_rasterio(land_fp)\n\n# Import Thomas Fire perimeter\nthomas_fire_perimeter = gpd.read_file(thomas_fp)\n\n\n\n\nIn this section we will take a preliminary look at the imported landsat data - in order to understand how to visualize it with respect to the Thomas Fire perimenter data.\n\n# View landsat\nlandsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870, band: 1)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0\nData variables:\n    red          (band, y, x) float64 ...\n    green        (band, y, x) float64 ...\n    blue         (band, y, x) float64 ...\n    nir08        (band, y, x) float64 ...\n    swir22       (band, y, x) float64 ...xarray.DatasetDimensions:y: 731x: 870band: 1Coordinates: (4)y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])band(band)int641array([1])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Attributes: (0)\n\n\n\n# View landsat sizes\nlandsat.sizes\n\nFrozen({'y': 731, 'x': 870, 'band': 1})\n\n\n\n# Landsat CRS\nlandsat.rio.crs\n\nCRS.from_epsg(32611)\n\n\nThrough the exploration of the “landsat” xarray.Dataset, I have been able to notice the shape (5, 1, 731, 870) and the dimensions (x coordinates, y coordinates and 1 band). The data’s variables has 5 groups of information - red, green, blue, near infrared 08, and short wave infrared 22. The crs of the dataset is ESPG: 32611.\n\n\n\nIn this section, we will remove the band dimension of the Landsat data (as there is only 1).\n\n# Drop the band dimension of the data\nlandsat = landsat.squeeze()\n\n# Remove coordinates associated to band dimension\nlandsat = landsat.drop_vars('band')\n\n# Check new landsat dataset\nprint(landsat.dims, landsat.coords)\n\nFrozen({'y': 731, 'x': 870}) Coordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n    spatial_ref  int64 0\n\n\n\n\n\nIn this section, I will make some preliminary visuals of the landsat data. I will be making true and false color images.\n\n\n\n# Adjust the scale for plotting the bands for a true color image\nfig, ax = plt.subplots(figsize = (5, 5)) # Set up plot\n\n(landsat[['red', 'green', 'blue']]).to_array().plot.imshow(robust = True) # Plot Landsat True Color image\n\nax.set_title(\"Santa Barbara County (True Color Image)\")  # Add title\n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Show map\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot false color image\nfig, ax = plt.subplots(figsize = (5, 5)) # Set up plot\n\n(landsat[['swir22', 'nir08', 'red']]).to_array().plot.imshow(robust = True) # Plot landsat false color image\n\n# Add title\nax.set_title(\"Santa Barbara County (False Color Image)\") \n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Show map\nplt.show()\n\n\n\n\n\n\n\n\nNotice how, in the false color image, the area approximating the Thomas Fires are a orange-red color compared to the rest of the green in the county.\n\n\n\n\nIn this final section, we will create maps of Santa Barbara overlayed with the Thomas Fire perimeter data (that we obtained in the fire perimeter notebook).\n\n# Set CRS of landsat and thomas_fire_perimeter equivalent\nthomas_fire_perimeter = thomas_fire_perimeter.to_crs(crs = landsat.rio.crs)\n\n# Test if equivalencies true\nassert thomas_fire_perimeter.crs == landsat.rio.crs\n\n\n# Create bounding box for fire perimeters\nlandsat_bounded = landsat.rio.clip_box(*thomas_fire_perimeter.total_bounds)\n\n# Clip map to fire perimeter bounds\nfig, ax = plt.subplots(figsize = (10, 10)) # Setup plot\nlandsat_bounded[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax = ax, robust = True) # Plot the landsat bands\n\nthomas_fire_perimeter.boundary.plot(ax = ax, edgecolor = '#5F0A11', linewidth = 2, label=\"Thomas Fire Boundary\") # Plot the thomas fire data on same plot\n\n# Add title\nax.set_title(\"Thomas Fire Burn Perimeter (2017) in Santa Barbara County (False Color Image)\")  \n\n# Set legend within map borders\nax.legend()\n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Set legend\nax.legend()\n\n# Show map\nplt.show()\n\n/opt/python/3.7.13/lib/python3.7/site-packages/geopandas/plotting.py:51: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n  if geom is not None and geom.type.startswith(prefix) and not geom.is_empty:\n\n\n\n\n\n\n\n\n\n\n\nThe 2 maps in the “Map” portion of this notebook, shows Santa Barbara County with fase color imagery - which include short wave infrared, near infrared, and red bands. The 2017 Thomas Fire burn perimeter is outlined in dark red, on top of this false image map, and it also appears red inside the perimeter (because of the false color imaging). This is due to the fact that it highly reflects shortwave infared. The Thomas Fire started 12/4/2017 and was contained on 1/12/2018. This imagery is therefore taken shortly after the fire was contained, on 1/26/2018.\n\n\n\n\n\n\nAQI Data : AirNow.gov, U.S. EPA. (n.d.-b). Aqi Basics. AQI Basics | AirNow.gov. https://www.airnow.gov/aqi/aqi-basics/\nThomas Fire Perimeter: Publisher CAL FIRE. (2024, May 14). State of California - california fire perimeters (all). Catalog. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436\nAssignment Reference and Cleaned Landsat Data Access : Galaz-Garcia, C. (n.d.). Assignment 4. assignment4 – EDS 220 - Working with Environmental Datasets. https://meds-eds-220.github.io/MEDS-eds-220-course/assignments/assignment4.html\nLandsat Data: Microsoft Planetary Computer. Planetary Computer. (2018, January 12). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html",
    "href": "posts/2024-10-18-my-first-post/index.html",
    "title": "blog post title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "title": "blog post title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "title": "blog post title",
    "section": "This is my second",
    "text": "This is my second\nHere’s my next paragraph2\nI’m citing me(online?)"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#cite-a-google-article-now",
    "href": "posts/2024-10-18-my-first-post/index.html#cite-a-google-article-now",
    "title": "blog post title",
    "section": "Cite a google article now",
    "text": "Cite a google article now\nHere is more rando text- this is a journal article cite now (Gaynor et al. 2022)"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "href": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "title": "blog post title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is a new footnote↩︎\nHere is my second footnote↩︎"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/index.html",
    "href": "posts/2024-12-01-thomas-fire-sb/index.html",
    "title": "Thomas Fire SB",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/index.html#this-is-my-first-section",
    "href": "posts/2024-12-01-thomas-fire-sb/index.html#this-is-my-first-section",
    "title": "Thomas Fire SB",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/index.html#this-is-my-second",
    "href": "posts/2024-12-01-thomas-fire-sb/index.html#this-is-my-second",
    "title": "Thomas Fire SB",
    "section": "This is my second",
    "text": "This is my second\nHere’s my next paragraph2\nI’m citing me[@online]"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/index.html#cite-a-google-article-now",
    "href": "posts/2024-12-01-thomas-fire-sb/index.html#cite-a-google-article-now",
    "title": "Thomas Fire SB",
    "section": "Cite a google article now",
    "text": "Cite a google article now\nHere is more rando text- this is a journal article cite now [@gaynor2022]"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/index.html#footnotes",
    "href": "posts/2024-12-01-thomas-fire-sb/index.html#footnotes",
    "title": "Thomas Fire SB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is a new footnote↩︎\nHere is my second footnote↩︎"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post-2.html",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post-2.html",
    "title": "Thomas Fire SB",
    "section": "",
    "text": "# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read in data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', compression = 'zip')\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', compression = 'zip')\n\n# Concatenate both dataframes\naqi = pd.concat([aqi_17, aqi_18])\n\n# Simplify column names\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Create filter Santa Barbara df with dropped columns\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code','county_code'], axis = 1)\n               \n# Change 'date' to datetime and set to index\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n               \n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n               \n# Create column with new variable 'rolling_variable'\naqi_sb['five_day_average'] = rolling_average\n               \n# Create plot with daily AQI and 5-day averages\ndaily_aqi_plot = aqi_sb[['aqi','five_day_average']].plot(title = 'Daily and 5-Day Average AQI in Santa Barbara County',\n                                  xlabel = 'Date',\n                                  ylabel = 'AQI')\n\n\n\n\n\n\n\n\nFire perimeter data retrieval and selection\nAuthor: Naomi Moraes\nLink: https://github.com/nmoraescommit/eds220-hw4/tree/main\nAbout\nPurpose: The purpose of this notebook is to explore, clean, and analyze the California fire perimeter shapefile, published by CAL FIRE. This is to obtain the Thomas Fire perimeter boundary for use in “hwk4-task2-false-color-MORAES”. Highlights: Working with this dataset was illuminating in looking at how state agencies store fire data and the aspects state agecies deem important to record, the fact that there is a start and end date to the observations, as well as learning how to store updated shape files. I consider the practice in setting up an entirely new project from scratch to be quite valuable, along with the process of independantly learning to access data for a continous workflow, important. About the data: This dataset was published and maintained by CAL FIRE, but accessed through Data.gov. The statewide fire history geospatial dataset is updated annually from the previous fire season, during spring, from units across the state and cooperating agencies. The first version was released in May 2015 - according the the CalFire site. References: Fire perimeter data: Publisher CAL FIRE. (2024, May 14). State of California - california fire perimeters (all). Catalog. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436 Assignment Reference and Cleaned Landsat Data Access : Galaz-Garcia, C. (n.d.). Assignment 4. assignment4 – EDS 220 - Working with Environmental Datasets. https://meds-eds-220.github.io/MEDS-eds-220-course/assignments/assignment4.html\nSet-Up In this section we will import the appropriate libraries and data to complete this workbook.\nImport Libraries\n\n# Import libraries\nimport os\nimport pandas as pd\nimport geopandas as gpd\n\n# Display all columns when looking at dataframes\npd.set_option(\"display.max.columns\", None)\n\nImport Data\n\n# Create data filepath\nfp = os.path.join('data','California_Fire_Perimeters_(all).shp')\n\n# Create dataframe for CA fire perimeter shapefile\nca_fire_perimeter = gpd.read_file(fp)\n\nExplore Data In this section we will take a preliminary look at the imported fire perimeter data - in order to understand how to extract the Thomas Fire perimenter data.\n\n# Check dataframe head\nca_fire_perimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNaN\nNaN\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNaN\nNaN\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNaN\nNaN\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNaN\nNaN\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNaN\nNaN\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNaN\nNaN\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Check dataframe tail\nca_fire_perimeter.tail(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n22258\n0\nCA\nCCO\nMRN\nUKNOWN\nNaN\n1899-12-30\n1899-12-30\n14\n6\n1\n2927.2400\n1917-34(Yr Not Report)MarinCo FireChief Garber...\nNaN\nNaN\nNaN\nNaN\n0\nPOLYGON ((-13658666.186 4605853.097, -13658738...\n\n\n22259\n0\nCA\nCCO\nMRN\nUKNOWN\nNaN\n1899-12-30\n1899-12-30\n14\n6\n1\n62.0127\n1917-34(Yr Not Report)MarinCo FireChief Garber...\nNaN\nNaN\nNaN\nNaN\n0\nPOLYGON ((-13644249.827 4580277.586, -13644243...\n\n\n22260\n0\nCA\nCCO\nMRN\nUKNOWN\nNaN\n1899-12-30\n1899-12-30\n14\n6\n1\n40.0137\n1917-34(Yr Not Report)MarinCo FireChief Garber...\nNaN\nNaN\nNaN\nNaN\n0\nPOLYGON ((-13640708.376 4580839.378, -13640603...\n\n\n\n\n\n\n\n\n# Check columns\nca_fire_perimeter.columns\n\nIndex(['YEAR_', 'STATE', 'AGENCY', 'UNIT_ID', 'FIRE_NAME', 'INC_NUM',\n       'ALARM_DATE', 'CONT_DATE', 'CAUSE', 'C_METHOD', 'OBJECTIVE',\n       'GIS_ACRES', 'COMMENTS', 'COMPLEX_NA', 'IRWINID', 'FIRE_NUM',\n       'COMPLEX_ID', 'DECADES', 'geometry'],\n      dtype='object')\n\n\n\n# Check column datatypes\nca_fire_perimeter.dtypes\n\nYEAR_            int64\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE            int64\nC_METHOD         int64\nOBJECTIVE        int64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nIRWINID         object\nFIRE_NUM        object\nCOMPLEX_ID      object\nDECADES          int64\ngeometry      geometry\ndtype: object\n\n\n\n# Check CRS - and type\nca_fire_perimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nAfter the exploration of the California perimeter shape file, imported into this notebook, I see that is has 22,260 fire perimeter observations with columns for varied descriptive markers including: year, fire name, alarm date, and geometery. I observe that some of the column data types int64 and float64, however, I may want to change the date columns into datetime objects for manipulation. The CRS of this shapefile is a projected coordinate reference system, EPSG: 3857 and is a popular crs for web mapping services.\nClean Data In this section, I will convert the column names to a lower snake case, as well as the date columns to DateTime objects - for ease of future data manipulation.\n\n# Simplify column names by replacing spaces and no capitilization\nca_fire_perimeter.columns = (ca_fire_perimeter.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Make dates into DateTime object\nca_fire_perimeter.alarm_date = pd.to_datetime(ca_fire_perimeter.alarm_date)\nca_fire_perimeter.cont_date = pd.to_datetime(ca_fire_perimeter.cont_date)\n\nThomas Fire Boundary Here, I will select for the Thomas Fire Boundary (2017), and save it as a new geospatial file.\n\n# Select Thomas Fire in 2017\nthomas_fire_boundary = ca_fire_perimeter[(ca_fire_perimeter['alarm_date'] &gt; '2016-12-31') & \n                                         (ca_fire_perimeter['alarm_date'] &lt; '2018-01-01') &\n                                         (ca_fire_perimeter['fire_name'] == 'THOMAS')]\n\n\n# View dataframe\nthomas_fire_boundary\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNaN\nNaN\nNaN\nNaN\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\n\n# Save dataframe as geospatial file in /data folder\nthomas_fire_boundary.to_file('data/thomas_fire_boundary.geojson', driver = 'GeoJSON')\n\nFile Shape Explanation I chose to convert the alarm_date and cont_date variables into DateTime objects, and wanted them to retain that data type. As I would need to convert DateTime objects back into strings to save as shapefile, I chose to store the new data frame as a GeoJSON file.\n\n\n\nCitationBibTeX citation:@online{moraes2024,\n  author = {Moraes, Naomi},\n  title = {Thomas {Fire} {SB}},\n  date = {2024-10-18},\n  url = {https://nmoraescommit.github.io/blog/2024-12-01-thomas-fire-sb},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMoraes, Naomi. 2024. “Thomas Fire SB.” October 18, 2024. https://nmoraescommit.github.io/blog/2024-12-01-thomas-fire-sb."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#import-libraries",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#import-libraries",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "Importing all relevant libraries - for all excersizes in this blog post.\n\n# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport rioxarray as rioxr\nimport xarray as xr\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#air-quality-index-analysis",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#air-quality-index-analysis",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "# Read in data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', compression = 'zip')\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', compression = 'zip')\n\n# Concatenate both dataframes\naqi = pd.concat([aqi_17, aqi_18])\n\n# Simplify column names\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Create filter Santa Barbara df with dropped columns\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code','county_code'], axis = 1)\n               \n# Change 'date' to datetime and set to index\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n               \n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n               \n# Create column with new variable 'rolling_variable'\naqi_sb['five_day_average'] = rolling_average\n               \n# Create plot with daily AQI and 5-day averages\ndaily_aqi_plot = aqi_sb[['aqi','five_day_average']].plot(title = 'Daily and 5-Day Average AQI in Santa Barbara County',\n                                  xlabel = 'Date',\n                                  ylabel = 'AQI')"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#fire-perimeter-data-retrieval-and-selection",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#fire-perimeter-data-retrieval-and-selection",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "The purpose of this notebook is to explore, clean, and analyze the California fire perimeter shapefile, published by CAL FIRE. This is to obtain the Thomas Fire perimeter boundary for use in “hwk4-task2-false-color-MORAES”.\nHighlights: Working with this dataset was illuminating in looking at how state agencies store fire data and the aspects state agecies deem important to record, the fact that there is a start and end date to the observations, as well as learning how to store updated shape files. I consider the practice in setting up an entirely new project from scratch to be quite valuable, along with the process of independantly learning to access data for a continous workflow, important.\nAbout the data: This dataset was published and maintained by CAL FIRE, but accessed through Data.gov. The statewide fire history geospatial dataset is updated annually from the previous fire season, during spring, from units across the state and cooperating agencies. The first version was released in May 2015 - according the the CalFire site.\n\n\n\n\n\n\n# Display all columns when looking at dataframes\npd.set_option(\"display.max.columns\", None)\n\n\n# Create data filepath\nfp = os.path.join('data','California_Fire_Perimeters_(all).shp')\n\n# Create dataframe for CA fire perimeter shapefile\nca_fire_perimeter = gpd.read_file(fp)\n\n\n\n\nIn this section we will take a preliminary look at the imported fire perimeter data - in order to understand how to extract the Thomas Fire perimenter data.\n\n# Check dataframe head\nca_fire_perimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNaN\nNaN\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNaN\nNaN\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNaN\nNaN\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNaN\nNaN\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNaN\nNaN\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNaN\nNaN\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Check CRS - and type\nca_fire_perimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nAfter the exploration of the California perimeter shape file, imported into this notebook, I see that is has 22,260 fire perimeter observations with columns for varied descriptive markers including: year, fire name, alarm date, and geometery. I observe that some of the column data types int64 and float64, however, I may want to change the date columns into datetime objects for manipulation. The CRS of this shapefile is a projected coordinate reference system, EPSG: 3857 and is a popular crs for web mapping services.\n\n\n\nNext, I will clean the data for easier data manipulation.\n\n# Simplify column names by replacing spaces and no capitilization\nca_fire_perimeter.columns = (ca_fire_perimeter.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\n\n# Make dates into DateTime object\nca_fire_perimeter.alarm_date = pd.to_datetime(ca_fire_perimeter.alarm_date)\nca_fire_perimeter.cont_date = pd.to_datetime(ca_fire_perimeter.cont_date)\n\n\n\n\nHere, I will select for the Thomas Fire Boundary (2017), and save it as a new geospatial file.\n\n# Select Thomas Fire in 2017\nthomas_fire_boundary = ca_fire_perimeter[(ca_fire_perimeter['alarm_date'] &gt; '2016-12-31') & \n                                         (ca_fire_perimeter['alarm_date'] &lt; '2018-01-01') &\n                                         (ca_fire_perimeter['fire_name'] == 'THOMAS')]\n\n\n# View dataframe\nthomas_fire_boundary\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNaN\nNaN\nNaN\nNaN\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\n\n# Save dataframe as geospatial file in /data folder\nthomas_fire_boundary.to_file('data/thomas_fire_boundary.geojson', driver = 'GeoJSON')\n\nI chose to convert the alarm_date and cont_date variables into DateTime objects, and wanted them to retain that data type. As I would need to convert DateTime objects back into strings to save as shapefile, I chose to store the new data frame as a GeoJSON file. I have saved the data in the ‘data/’ folder common to this project."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#false-and-true-color-images",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#false-and-true-color-images",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "In a new notebook, I have created a path to the Thomas Fire perimeter data saved in the previous step.\n\n\n\n# Set up file paths\nland_fp = os.path.join('data', 'landsat8-2018-01-26-sb-simplified.nc')\nthomas_fp = os.path.join('data', 'thomas_fire_boundary.geojson')\n\n# Import landsat data\nlandsat = rioxr.open_rasterio(land_fp)\n\n# Import Thomas Fire perimeter\nthomas_fire_perimeter = gpd.read_file(thomas_fp)\n\n\n\n\nIn this section we will take a preliminary look at the imported landsat data - in order to understand how to visualize it with respect to the Thomas Fire perimenter data.\n\n# View landsat\nlandsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870, band: 1)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0\nData variables:\n    red          (band, y, x) float64 ...\n    green        (band, y, x) float64 ...\n    blue         (band, y, x) float64 ...\n    nir08        (band, y, x) float64 ...\n    swir22       (band, y, x) float64 ...xarray.DatasetDimensions:y: 731x: 870band: 1Coordinates: (4)y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])band(band)int641array([1])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Attributes: (0)\n\n\n\n# View landsat sizes\nlandsat.sizes\n\nFrozen({'y': 731, 'x': 870, 'band': 1})\n\n\n\n# Landsat CRS\nlandsat.rio.crs\n\nCRS.from_epsg(32611)\n\n\nThrough the exploration of the “landsat” xarray.Dataset, I have been able to notice the shape (5, 1, 731, 870) and the dimensions (x coordinates, y coordinates and 1 band). The data’s variables has 5 groups of information - red, green, blue, near infrared 08, and short wave infrared 22. The crs of the dataset is ESPG: 32611.\n\n\n\nIn this section, we will remove the band dimension of the Landsat data (as there is only 1).\n\n# Drop the band dimension of the data\nlandsat = landsat.squeeze()\n\n# Remove coordinates associated to band dimension\nlandsat = landsat.drop_vars('band')\n\n# Check new landsat dataset\nprint(landsat.dims, landsat.coords)\n\nFrozen({'y': 731, 'x': 870}) Coordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n    spatial_ref  int64 0\n\n\n\n\n\nIn this section, I will make some preliminary visuals of the landsat data. I will be making true and false color images.\n\n\n\n# Adjust the scale for plotting the bands for a true color image\nfig, ax = plt.subplots(figsize = (5, 5)) # Set up plot\n\n(landsat[['red', 'green', 'blue']]).to_array().plot.imshow(robust = True) # Plot Landsat True Color image\n\nax.set_title(\"Santa Barbara County (True Color Image)\")  # Add title\n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Show map\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot false color image\nfig, ax = plt.subplots(figsize = (5, 5)) # Set up plot\n\n(landsat[['swir22', 'nir08', 'red']]).to_array().plot.imshow(robust = True) # Plot landsat false color image\n\n# Add title\nax.set_title(\"Santa Barbara County (False Color Image)\") \n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Show map\nplt.show()\n\n\n\n\n\n\n\n\nNotice how, in the false color image, the area approximating the Thomas Fires are a orange-red color compared to the rest of the green in the county.\n\n\n\n\nIn this final section, we will create maps of Santa Barbara overlayed with the Thomas Fire perimeter data (that we obtained in the fire perimeter notebook).\n\n# Set CRS of landsat and thomas_fire_perimeter equivalent\nthomas_fire_perimeter = thomas_fire_perimeter.to_crs(crs = landsat.rio.crs)\n\n# Test if equivalencies true\nassert thomas_fire_perimeter.crs == landsat.rio.crs\n\n\n# Create bounding box for fire perimeters\nlandsat_bounded = landsat.rio.clip_box(*thomas_fire_perimeter.total_bounds)\n\n# Clip map to fire perimeter bounds\nfig, ax = plt.subplots(figsize = (10, 10)) # Setup plot\nlandsat_bounded[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax = ax, robust = True) # Plot the landsat bands\n\nthomas_fire_perimeter.boundary.plot(ax = ax, edgecolor = '#5F0A11', linewidth = 2, label=\"Thomas Fire Boundary\") # Plot the thomas fire data on same plot\n\n# Add title\nax.set_title(\"Thomas Fire Burn Perimeter (2017) in Santa Barbara County (False Color Image)\")  \n\n# Set legend within map borders\nax.legend()\n\n# Set axis labels\nax.set_xlabel(\"Longitude\") \nax.set_ylabel(\"Latitude\")\n\n# Set legend\nax.legend()\n\n# Show map\nplt.show()\n\n/opt/python/3.7.13/lib/python3.7/site-packages/geopandas/plotting.py:51: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n  if geom is not None and geom.type.startswith(prefix) and not geom.is_empty:\n\n\n\n\n\n\n\n\n\n\n\nThe 2 maps in the “Map” portion of this notebook, shows Santa Barbara County with fase color imagery - which include short wave infrared, near infrared, and red bands. The 2017 Thomas Fire burn perimeter is outlined in dark red, on top of this false image map, and it also appears red inside the perimeter (because of the false color imaging). This is due to the fact that it highly reflects shortwave infared. The Thomas Fire started 12/4/2017 and was contained on 1/12/2018. This imagery is therefore taken shortly after the fire was contained, on 1/26/2018."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#references",
    "href": "posts/2024-12-01-thomas-fire-sb/Thomas_Fire_Blog_Post.html#references",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "AQI Data : AirNow.gov, U.S. EPA. (n.d.-b). Aqi Basics. AQI Basics | AirNow.gov. https://www.airnow.gov/aqi/aqi-basics/\nThomas Fire Perimeter: Publisher CAL FIRE. (2024, May 14). State of California - california fire perimeters (all). Catalog. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436\nAssignment Reference and Cleaned Landsat Data Access : Galaz-Garcia, C. (n.d.). Assignment 4. assignment4 – EDS 220 - Working with Environmental Datasets. https://meds-eds-220.github.io/MEDS-eds-220-course/assignments/assignment4.html\nLandsat Data: Microsoft Planetary Computer. Planetary Computer. (2018, January 12). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2"
  },
  {
    "objectID": "posts/2024-12-13-statistics-final/heat_income_sanjose.html",
    "href": "posts/2024-12-13-statistics-final/heat_income_sanjose.html",
    "title": "Investigating Temperature and Income in San Jose",
    "section": "",
    "text": "My starting point was quite ambitious - especially for the time span I had allotted myself for this project. A mere 2 weeks! Gathering and cleaning data, creating code to process it, analyzing and visualizing the results - to look at incomes and temperatures around the entirety of the Bay Area was no small task. Furthermore, due to the inland spread of certain counties, like Alameda, inland, there were areas where there would naturally be higher temperatures that may skew the research. \nThus, looking at my available data sources of the NOAA local temperature data as well as the data made available by the tidycensus library in R, I decided to zoom in on the city of San Jose, in the South Bay.\nThrough a series of questions, I wanted to get an introductory understanding of the relationship between temperature and median household income, in San Jose, during the years 2005 - 2009. According to articles I have read [CITE COME ARTICLES], there is a “{QUOTE]” - relationship between heat and income. (And maybe disproportionate care between poor and heat?” Through data analysis, I wanted to observe if these assumptions held up in the specific area I was investigating. \nAfter cleaning my temperature data and income data, I was left with the following independent variables: HourlyDryBulbTemperature (which represented the temperature taken during the hour (24 hours or not?)), and (average?) median household income per census tract (in what kind of dollars).\nTo provide a summary of the median household income of the census tracts surrounding each of the stations, I used geospatial tools in R. I created geographic points for each of the stations and created a buffer zone of 4500 m (creating a circle around each point of x DIAMETER). Overlaying these polygons onto the Santa Clara census tract shapefile, I used the [NAME] library to find where the buffered zones of each station  intersected with the census tract shapefiles. Using the GEOIDs of the intersected census tracts as the joining feature, I created a new joint data table of the identified census tracts and their respective median incomes, by joining them to the census [NAME] ACS 5 year.\nThen, after finding the respective means of the median incomes surrounding each station, I joined this information to the finalized data frame. I repeated this process for 4 times - with ACS 5 year data for the years 2009, 2014, 2019, and 2022 (as the data for 2023 was unavailable) as well as with the census tract shapefiles for the same years. (I did this to ensure that changing census tracts boundaries and labels over the time-period, would not skew the resulting mean, median income household data surrounding each station.)\nThe resulting data frame, required for the analysis, contained the mean, median income surrounding each station for a given year. Given the time constraints, I assumed that the income variable would be applicable to a five year time period (e.g. the 2009 ACS median household income would be applied to the years of 2005 to 2009.)\nI also summarised the temperature data, by selecting for the highest temperature within the day in order to find the maximum daily temperature over the year 2005 - 2000. Furthermore, I found the days where the temperature was above 90 degrees celsius, over the time period, for all stations.\n\n\n\nDescribe your data. Where did you access it? What are its spatial and temporal features? What are its limitations? What do you know about the sampling strategy and what biases that may introduce? If helpful, you can use a histogram, scatterplot, or summary statistics table to describe your data.\n\nTemperature data across the city of San Jose, was retrieved from the National Centers for Environmental Infomation, National Oceanic and Atmospheric Administration (NOAA) climate data online, specifically their local climatological data. Selecting the county of interest as Santa Clara, we  \nThis was because there were three stations, quite nicely spread out on a diagonal diameter of the city, which allowed me to gather a temperature data I believed to be representative of a cross-section of the city. The time-range for the recorded historical temperatures included  the time-range that I wanted to observe, 2010 - 2024.\n\n\nSummarize your results visually and in words. Show us your results in figure(s) and/or table(s) that are carefully labeled and captioned. Describe in the text (and orally when presenting) what you found, and how these results either do or do not help you answeryour question.\nEvery year, around the months of July through to August, I have noticed an uptick in news articles and government advisories cautioning people about the dangers of increasingly frequent heat waves. (Heatwaves can cause health-related problems, like heat strokes, but it can also increase the burden on water, energy, and transportation services - causing blackouts and gridlock that further exacerbate the problem. (Heatwaves, n.d.)) As a Bay Area native, and a burgeoning data scientist, with an interest in environmental justice, I wanted to explore the relationship between temperature, income, and location in the Bay Area. (This blog post is an extension of my final project presentation for grad school, for the course EDS222.) Through this investigation, I pose multiple questions to try and get a holistic understanding of the topic.\nMy starting point was quite ambitious - especially for the time span I had allotted myself for this project. A mere 2 weeks! Gathering and cleaning data, creating code to process it, analyzing and visualizing the results - to look at incomes and temperatures around the entirety of the Bay Area was no small task. Furthermore, due to the inland spread of certain counties, like Alameda, inland, there were areas where there would naturally be higher temperatures that may skew the research. \nThus, looking at my data sources of the NOAA local temperature data as well as the data made available by the tidycensus library in R, I decided to zoom in on the city of San Jose, in the South Bay.\nTemperature data across the city of San Jose, was retrieved from the National Centers for Environmental Information, National Oceanic and Atmospheric Administration (NOAA). I utilised the climate data online tool, to pull local climatological data from my county of interest - Santa Clara. While there are a total of four stations in the area, the three stations I found the most promising in terms of recorded time range, location, and geographic relevance were the Moffett Federal Airfield, San Jose Mineta National Airport, and the Reid-Hillview Airport.\nThis was because there were three stations, quite nicely spread out on a diagonal diameter of the city, which allowed me to gather a temperature data I believed to be representative of a cross-section of the city. The time-range for the recorded historical temperatures included  the time-range that I wanted to observe, 2010 - 2024.\nThe relevant temperature data was downloaded from the website, and was saved as three .csv’s to my local drive. After creating a new GitHub repository [NAME], I cloned it to my IDE and uploaded the .csv’s to the data folder (already placed within the .gitignore of the repository.) I created a script that would clean the data available in [NOTEBOOK NAME]. In this process, the temperature data I thought most useful was the ‘HourlyDryBulbTemperature’. While there was a ‘DailyMaximumDryBulbTemperature’, ‘DailyMaximumWetBulbTemperature’, and ‘HourlyDryBulbTemperature’ it was not as consistent or as complete as the chosen variable.\nI further processed the .csv data in my R script [NAME] also available on the same GitHub repository. I selected the highest hourly temperature, per day, and was able to approximate the highest daily dry bulb temperature for the years 2005 - 2024 for all three stations. \nThe geographic shapefiles, for census tracts within Santa Clara County, were pulled from the TIGER/Shapeline files from the US Census Bureau [CITE]. The shapefiles were downloaded as shapefiles??? And uploaded to the data folder [NAME] within the .gitignore. I used the census tract data for the years 2009, 2014, 2019, and 2023. This was to match the GEOIDs and areas of the census tracts used within the 5 year ACS census.\nFinally, I required median household income data, which I retrieved using the tidycensus library in R. I retrieved the 5 year ACS census [NAME] for the years 2005 - 2009, 2010 - 2014, 2015 - 2019, and 2020  ?????? \nWhile I would have liked to use the median incomes of every year within the, from 2005 - 2009, this amount of data processing was not feasible given the time and scope of this project. \nhrough a series of questions, I wanted to get an introductory understanding of the relationship between temperature and median household income, in San Jose, during the years 2005 - 2009. According to articles I have read [CITE COME ARTICLES], there is a “{QUOTE]” - relationship between heat and income. (And maybe disproportionate care between poor and heat?” Through data analysis, I wanted to observe if these assumptions held up in the specific area I was investigating. \nAfter cleaning my temperature data and income data, I was left with the following independent variables: HourlyDryBulbTemperature (which represented the temperature taken during the hour (24 hours or not?)), and (average?) median household income per census tract (in what kind of dollars).\nTo provide a summary of the median household income of the census tracts surrounding each of the stations, I used geospatial tools in R. I created geographic points for each of the stations and created a buffer zone of 4500 m (creating a circle around each point of x DIAMETER). Overlaying these polygons onto the Santa Clara census tract shapefile, I used the [NAME] library to find where the buffered zones of each station  intersected with the census tract shapefiles. Using the GEOIDs of the intersected census tracts as the joining feature, I created a new joint data table of the identified census tracts and their respective median incomes, by joining them to the census [NAME] ACS 5 year.\nThen, after finding the respective means of the median incomes surrounding each station, I joined this information to the finalized data frame. I repeated this process for 4 times - with ACS 5 year data for the years 2009, 2014, 2019, and 2022 (as the data for 2023 was unavailable) as well as with the census tract shapefiles for the same years. (I did this to ensure that changing census tracts boundaries and labels over the time-period, would not skew the resulting mean, median income household data surrounding each station.)\nThe resulting data frame, required for the analysis, contained the mean, median income surrounding each station for a given year. Given the time constraints, I assumed that the income variable would be applicable to a five year time period (e.g. the 2009 ACS median household income would be applied to the years of 2005 to 2009.)\nI also summarised the temperature data, by selecting for the highest temperature within the day in order to find the maximum daily temperature over the year 2005 - 2000. Furthermore, I found the days where the temperature was above 90 degrees celsius, over the time period, for all stations.\nSummarize your results visually and in words. Show us your results in figure(s) and/or table(s) that are carefully labeled and captioned. Describe in the text (and orally when presenting) what you found, and how these results either do or do not help you answeryour question.\nNow that I had all my relevant cleaned and processed, I thought about how I would go about \nWhat might you do next? One analysis cannot fully answer an interesting scientific question. If you had time to collect more data or conduct more analyses, what would help you answer this question better? (Make sure to answer about bias - personal and in data….)\n\n\n\n\nCitationBibTeX citation:@online{moraes2024,\n  author = {Moraes, Naomi},\n  title = {Investigating {Temperature} and {Income} in {San} {Jose}},\n  date = {2024-12-12},\n  url = {https://nmoraescommit.github.io/blog/2024-10-18-my-first-post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMoraes, Naomi. 2024. “Investigating Temperature and Income in San\nJose.” December 12, 2024. https://nmoraescommit.github.io/blog/2024-10-18-my-first-post."
  }
]